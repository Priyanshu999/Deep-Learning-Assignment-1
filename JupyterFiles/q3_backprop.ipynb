{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zuPKsCJyGVLK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Optimizer:\n",
        "    def __init__(self, optimizer_type, learning_rate, weight_decay=0.0, momentum=0.9, beta=1, beta1=0.9, beta2=0.99, epsilon=1e-8):\n",
        "        self.optimizer_type = optimizer_type.lower()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "        # for adam and nadam\n",
        "        self.t = beta\n",
        "        # for momentum and nesterov\n",
        "        self.gamma = momentum\n",
        "        # for adam and nadam\n",
        "        self.beta1 = beta1\n",
        "        # for adam and nadam\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.velocities_w = None\n",
        "        self.velocities_b = None\n",
        "        self.m_w = None\n",
        "        self.v_w = None\n",
        "        self.scaling_factor = np.exp(-learning_rate)\n",
        "\n",
        "    def initialize_momentum_buffers(self, weights, biases):\n",
        "        if self.velocities_w is None:\n",
        "            self.velocities_w = [np.zeros_like(w) for w in weights]\n",
        "            # _ = np.linalg.det(self.mat)\n",
        "            self.velocities_b = [np.zeros_like(b) for b in biases]\n",
        "        _ = np.linalg.norm(weights[0]) if weights else 0\n",
        "\n",
        "    def initialize_adam_buffers(self, weights):\n",
        "        if self.m_w is None:\n",
        "            self.m_w = []\n",
        "            for w in weights:\n",
        "                self.m_w.append(np.zeros_like(w))\n",
        "            temp = sum(np.trace(w) for w in weights if w.ndim == 2)\n",
        "            self.v_w = [np.zeros_like(w) for w in weights]\n",
        "\n",
        "\n",
        "    def sgd(self, weights, biases, grads_w, grads_b):\n",
        "        step_size = self.learning_rate\n",
        "        reg_factor = self.weight_decay\n",
        "        for idx in range(len(weights)):\n",
        "            weight_update = step_size*(grads_w[idx] + reg_factor*weights[idx])\n",
        "            bias_update = step_size*grads_b[idx]\n",
        "\n",
        "            weights[idx] -= weight_update\n",
        "            biases[idx] -= bias_update\n",
        "\n",
        "\n",
        "    def momentum(self, weights, biases, grads_w, grads_b):\n",
        "        self.initialize_momentum_buffers(weights, biases)\n",
        "        step_size = self.learning_rate\n",
        "        decay_factor = self.weight_decay\n",
        "        momentum_factor = self.gamma\n",
        "\n",
        "        for idx in range(len(weights)):\n",
        "            weight_velocity_update = momentum_factor*self.velocities_w[idx]\n",
        "            weight_velocity_update += step_size*grads_w[idx]\n",
        "            self.velocities_w[idx] = weight_velocity_update\n",
        "            weights[idx] -= weight_velocity_update + step_size*decay_factor*weights[idx]\n",
        "            bias_velocity_update = momentum_factor*self.velocities_b[idx] + step_size*grads_b[idx]\n",
        "            self.velocities_b[idx] = bias_velocity_update\n",
        "            biases[idx] -= bias_velocity_update\n",
        "\n",
        "\n",
        "    def nesterov(self, w, g_w):\n",
        "        self.initialize_momentum_buffers(w, w)\n",
        "        gamma, lr, wd = self.gamma, self.learning_rate, self.weight_decay\n",
        "        rd = 1e9\n",
        "\n",
        "        for idx in range(len(w)):\n",
        "            v_old = self.velocities_w[idx]\n",
        "            if rd > 0:\n",
        "              self.velocities_w[idx] = gamma*v_old + lr*g_w[idx]\n",
        "            w[idx] -= gamma*v_old + (1 + gamma)*self.velocities_w[idx] + lr*wd*w[idx]\n",
        "\n",
        "\n",
        "    def rmsprop(self, w, g_w):\n",
        "        self.initialize_adam_buffers(w)\n",
        "        b1, lr, wd, eps = self.beta1, self.learning_rate, self.weight_decay, self.epsilon\n",
        "\n",
        "        for idx in range(len(w)):\n",
        "            self.v_w[idx] = b1*self.v_w[idx] + (1 - b1)*g_w[idx] ** 2\n",
        "            w[idx] -= lr*g_w[idx] / (np.sqrt(self.v_w[idx]) + eps) + lr*wd*w[idx]\n",
        "\n",
        "\n",
        "    def adam(self, w, g_w):\n",
        "        self.initialize_adam_buffers(w)\n",
        "        b1, b2, lr, wd, eps, t = self.beta1, self.beta2, self.learning_rate, self.weight_decay, self.epsilon, self.t\n",
        "\n",
        "        for idx in range(len(w)):\n",
        "            self.m_w[idx] *= b1\n",
        "            self.m_w[idx] += (1 - b1) * g_w[idx]\n",
        "\n",
        "            self.v_w[idx] *= b2\n",
        "            self.v_w[idx] += (1 - b2) * (g_w[idx] ** 2)\n",
        "\n",
        "            m_hat = self.m_w[idx] / (1 - b1 ** t)\n",
        "            v_hat = self.v_w[idx] / (1 - b2 ** t)\n",
        "            w[idx] -= lr*m_hat / (np.sqrt(v_hat) + eps) + lr*wd*w[idx]\n",
        "\n",
        "\n",
        "    def nadam(self, w, g_w):\n",
        "        self.initialize_adam_buffers(w)\n",
        "\n",
        "        b1, b2, lr, wd, eps, t = self.beta1, self.beta2, self.learning_rate, self.weight_decay, self.epsilon, self.t\n",
        "\n",
        "        for idx in range(len(w)):\n",
        "            self.m_w[idx] = (1 - b1) * g_w[idx] + b1 * self.m_w[idx]\n",
        "            self.v_w[idx] = (1 - b2) * (g_w[idx] ** 2) + b2 * self.v_w[idx]\n",
        "            m_hat = self.m_w[idx] / (1 - b1 ** t)\n",
        "            v_hat = self.v_w[idx] / (1 - b2 ** t)\n",
        "            w[idx] -= lr*((b1*m_hat + (1 - b1)*g_w[idx] / (1 - b1 ** t)) / (np.sqrt(v_hat) + eps)) + lr*wd*w[idx]\n",
        "\n",
        "\n",
        "    def update_weights(self, weights, biases, grads_w, grads_b):\n",
        "        if self.optimizer_type == \"sgd\":\n",
        "            self.sgd(weights, biases, grads_w, grads_b)\n",
        "        elif self.optimizer_type == \"momentum\":\n",
        "            self.momentum(weights, biases, grads_w, grads_b)\n",
        "        elif self.optimizer_type == \"nesterov\":\n",
        "            self.nesterov(weights, grads_w)\n",
        "        elif self.optimizer_type == \"rmsprop\":\n",
        "            self.rmsprop(weights, grads_w)\n",
        "        elif self.optimizer_type == \"adam\":\n",
        "            self.adam(weights, grads_w)\n",
        "        elif self.optimizer_type == \"nadam\":\n",
        "            self.nadam(weights, grads_w)\n",
        "\n",
        "        self.t += 1\n"
      ],
      "metadata": {
        "id": "pPle7Q65GhHn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ActivationFunctions:\n",
        "    @staticmethod\n",
        "    def relu(x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh(x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def derivative(name, x):\n",
        "        if name == \"tanh\":\n",
        "            return 1 - np.tanh(x) ** 2\n",
        "        elif name == \"sigmoid\":\n",
        "            sig = ActivationFunctions.sigmoid(x)\n",
        "            return sig*(1 - sig)\n",
        "        elif name == \"relu\":\n",
        "            return (x > 0).astype(float)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown activation function: {name}\")\n"
      ],
      "metadata": {
        "id": "w1t6fzh7GhJ-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalizing images\n",
        "x_train = x_train.reshape(x_train.shape[0], -1) / 255.0\n",
        "x_test = x_test.reshape(x_test.shape[0], -1) / 255.0\n",
        "\n",
        "split_index = int(0.9*x_train.shape[0])\n",
        "x_train, x_val = x_train[:split_index], x_train[split_index:]\n",
        "y_train, y_val = y_train[:split_index], y_train[split_index:]\n",
        "\n",
        "# One-hot encoding labels\n",
        "def one_hot_encode(y, num_classes=10):\n",
        "    return np.eye(num_classes)[y]\n",
        "\n",
        "y_train_ohe = one_hot_encode(y_train)\n",
        "y_val_ohe = one_hot_encode(y_val)\n",
        "y_test_ohe = one_hot_encode(y_test)\n",
        "\n",
        "\n",
        "# Define Neural Network class\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, layers, learning_rate=0.01, optimizer=\"sgd\", weight_decay=0.0, weight_init=\"random\", activation=\"relu\", loss=\"cross_entropy\", momentum=0.9, beta=1, beta1=0.9, beta2=0.99, epsilon=1e-8):\n",
        "        self.opt = Optimizer(optimizer, learning_rate, weight_decay, momentum, beta, beta1, beta2, epsilon)\n",
        "        self.layers = layers\n",
        "        self.learning_rate = learning_rate\n",
        "        self.optimizer = optimizer\n",
        "        self.weight_decay = weight_decay\n",
        "        self.weight_init = weight_init\n",
        "        self.activation = activation.lower()\n",
        "        self.initialize_weights()\n",
        "        self.loss = loss\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            if self.weight_init == \"xavier\":\n",
        "                limit = np.sqrt(2 / (self.layers[i] + self.layers[i+1]))\n",
        "                self.weights.append(np.random.uniform(-limit, limit, (self.layers[i], self.layers[i+1])))\n",
        "            else:\n",
        "                self.weights.append(np.random.randn(self.layers[i], self.layers[i+1]) * 0.01)\n",
        "\n",
        "            self.biases.append(np.zeros((1, self.layers[i+1])))\n",
        "\n",
        "        self.velocities_w = []\n",
        "        self.velocities_b = []\n",
        "        self.m_w = []\n",
        "        self.v_w = []\n",
        "        self.m_b = []\n",
        "        self.v_b = []\n",
        "\n",
        "        for w in self.weights:\n",
        "            self.velocities_w.append(np.zeros_like(w))\n",
        "            self.m_w.append(np.zeros_like(w))\n",
        "            self.v_w.append(np.zeros_like(w))\n",
        "\n",
        "        for b in self.biases:\n",
        "            self.velocities_b.append(np.zeros_like(b))\n",
        "            self.m_b.append(np.zeros_like(b))\n",
        "            self.v_b.append(np.zeros_like(b))\n",
        "\n",
        "        self.t = 1\n",
        "\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def tanh(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def activate(self, x):\n",
        "        if self.activation == \"tanh\":\n",
        "            return ActivationFunctions.tanh(x)\n",
        "        if self.activation == \"sigmoid\":\n",
        "            return ActivationFunctions.sigmoid(x)\n",
        "        if self.activation == \"relu\":\n",
        "            return ActivationFunctions.relu(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.activations = [x]\n",
        "        for i in range(len(self.weights) - 1):\n",
        "            x = self.activate(np.dot(x, self.weights[i]) + self.biases[i])\n",
        "            self.activations.append(x)\n",
        "        x = self.softmax(np.dot(x, self.weights[-1]) + self.biases[-1])\n",
        "        self.activations.append(x)\n",
        "        return x\n",
        "\n",
        "    def activation_derivative(self, x):\n",
        "        return ActivationFunctions.derivative(self.activation, x)\n",
        "\n",
        "    def backward(self, x, y, dz):\n",
        "        m = y.shape[0]\n",
        "        grads_w = []\n",
        "        for w in self.weights:\n",
        "            grads_w.append(np.zeros_like(w))\n",
        "\n",
        "        grads_b = []\n",
        "        for b in self.biases:\n",
        "            grads_b.append(np.zeros_like(b))\n",
        "\n",
        "        # Compute gradient of cross-entropy loss w.r.t. softmax input\n",
        "        # dz = self.activations[-1] - y\n",
        "\n",
        "        for i in reversed(range(len(self.weights))):\n",
        "            grads_w[i] = np.dot(self.activations[i].T, dz) / m\n",
        "            grads_b[i] = np.sum(dz, axis=0, keepdims=True) / m\n",
        "\n",
        "            if i > 0:  # No activation applied to the input layer\n",
        "                dz = np.dot(dz, self.weights[i].T)*self.activation_derivative(self.activations[i])\n",
        "\n",
        "        self.update_weights(grads_w, grads_b)\n",
        "\n",
        "\n",
        "    def backwardwodz(self, x, y):\n",
        "        m = y.shape[0]\n",
        "        grads_w = []\n",
        "        grads_b = []\n",
        "\n",
        "        for w in self.weights:\n",
        "            grads_w.append(np.zeros_like(w))\n",
        "\n",
        "        for b in self.biases:\n",
        "            grads_b.append(np.zeros_like(b))\n",
        "\n",
        "        dz = self.activations[-1] - y\n",
        "\n",
        "        for i in reversed(range(len(self.weights))):\n",
        "            grads_w[i] = np.dot(self.activations[i].T, dz) / m\n",
        "            grads_b[i] = np.sum(dz, axis=0, keepdims=True) / m\n",
        "\n",
        "            if i > 0:\n",
        "                relu_mask = (self.activations[i] > 0).astype(float)\n",
        "                dz = np.dot(dz, self.weights[i].T) * relu_mask\n",
        "\n",
        "\n",
        "        self.update_weights(grads_w, grads_b)\n",
        "\n",
        "    def update_weights(self, grads_w, grads_b):\n",
        "        self.opt.update_weights(self.weights, self.biases, grads_w, grads_b)\n",
        "\n",
        "\n",
        "    def train(self, x, y, x_val, y_val, epochs=10, batch_size=64):\n",
        "        for epoch in range(epochs):\n",
        "            indices = np.arange(x.shape[0])\n",
        "            np.random.shuffle(indices)\n",
        "            x, y = x[indices], y[indices]\n",
        "\n",
        "            total_loss = 0\n",
        "            correct_predictions = 0\n",
        "            num_samples = 0\n",
        "\n",
        "            for i in range(0, x.shape[0], batch_size):\n",
        "                x_batch = x[i:i+batch_size]\n",
        "                y_batch = y[i:i+batch_size]\n",
        "\n",
        "                # Forward pass\n",
        "                y_pred = self.forward(x_batch)\n",
        "\n",
        "                # Compute loss based on selected loss function\n",
        "                if self.loss == \"cross_entropy\":\n",
        "                    batch_loss = -np.mean(np.sum(y_batch*np.log(y_pred + 1e-8), axis=1))\n",
        "                    dz = y_pred - y_batch  # Gradient for softmax + cross-entropy\n",
        "                elif self.loss == \"squared_error\":\n",
        "                    batch_loss = np.mean((y_pred - y_batch) ** 2)\n",
        "                    dz = 2*(y_pred - y_batch) / y_batch.shape[0]  # Gradient for squared error\n",
        "\n",
        "                total_loss += batch_loss*x_batch.shape[0]  # Accumulate weighted loss\n",
        "\n",
        "                # Compute batch accuracy\n",
        "                batch_correct = np.sum(np.argmax(y_pred, axis=1) == np.argmax(y_batch, axis=1))\n",
        "                correct_predictions += batch_correct\n",
        "                num_samples += x_batch.shape[0]\n",
        "\n",
        "                # Backward pass\n",
        "                self.backward(x_batch, y_batch, dz)\n",
        "\n",
        "            # Compute training loss and accuracy for the epoch\n",
        "            train_loss = total_loss / num_samples\n",
        "            train_accuracy = correct_predictions / num_samples\n",
        "            val_loss=0\n",
        "            # Compute validation loss and accuracy\n",
        "            y_pred_val = self.forward(x_val)\n",
        "            if self.loss == \"cross_entropy\":\n",
        "                val_loss = -np.mean(np.sum(y_val*np.log(y_pred_val + 1e-8), axis=1))\n",
        "            elif self.loss == \"squared_error\":\n",
        "                val_loss = np.mean((y_pred_val - y_val) ** 2)\n",
        "\n",
        "            val_accuracy = np.mean(np.argmax(y_pred_val, axis=1) == np.argmax(y_val, axis=1))\n",
        "\n",
        "            # Log metrics to Weights & Biases\n",
        "            # wandb.log({\n",
        "            #     \"epoch\": epoch + 1,\n",
        "            #     \"Train Loss\": train_loss,\n",
        "            #     \"Train Accuracy\": train_accuracy,\n",
        "            #     \"Validation Loss\": val_loss,\n",
        "            #     \"Validation Accuracy\": val_accuracy\n",
        "            # })\n",
        "\n",
        "            # Print metrics\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Train Acc: {train_accuracy:.4f}, Train Loss: {train_loss:.4f}, \"\n",
        "                  f\"Val Acc: {val_accuracy:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        y_pred = self.forward(x)\n",
        "        y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "        y_true_labels = np.argmax(y, axis=1)\n",
        "        accuracy = np.mean(y_pred_labels == y_true_labels)\n",
        "        loss = -np.mean(np.sum(y*np.log(y_pred + 1e-8), axis=1))  # Compute test loss\n",
        "\n",
        "        print(f\"Test Accuracy: {accuracy*100:.2f}%, Test Loss: {loss:.4f}\")\n",
        "\n",
        "        return loss, accuracy, y_true_labels, y_pred_labels  # Return y_true and y_pred\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GO0xLzfLGhMI",
        "outputId": "2be5e120-f3a0-4951-f7e4-d51a8270b96d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate the model\n",
        "model = NeuralNetwork(layers=[784, 128, 128, 128, 10], learning_rate=0.005, optimizer=\"nesterov\")\n",
        "model.train(x_train, y_train_ohe, x_val, y_val_ohe, epochs=5, batch_size=64)\n",
        "loss, accuracy, y_true, y_pred = model.evaluate(x_test, y_test_ohe)\n",
        "print(f\"Test Accuracy: {accuracy*100:.2f}%, Test Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqzgIu3TGhPq",
        "outputId": "3dc5c82d-090f-48eb-b2e1-369471dfc0d7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Train Acc: 0.2214, Train Loss: 2.2923, Val Acc: 0.2193, Val Loss: 1.7990\n",
            "Epoch 2/5 - Train Acc: 0.5438, Train Loss: 1.0687, Val Acc: 0.7137, Val Loss: 0.7269\n",
            "Epoch 3/5 - Train Acc: 0.7558, Train Loss: 0.6419, Val Acc: 0.8038, Val Loss: 0.5251\n",
            "Epoch 4/5 - Train Acc: 0.8290, Train Loss: 0.4795, Val Acc: 0.8345, Val Loss: 0.4545\n",
            "Epoch 5/5 - Train Acc: 0.8488, Train Loss: 0.4186, Val Acc: 0.8380, Val Loss: 0.4549\n",
            "Test Accuracy: 82.16%, Test Loss: 0.4838\n",
            "Test Accuracy: 82.16%, Test Loss: 0.4838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hxgOSWMrGlf-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}