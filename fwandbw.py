# -*- coding: utf-8 -*-
"""FwAndBw.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HRdPAkTxgKIC7CHS0ZWB22mVaeKH-fjq
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from keras.datasets import fashion_mnist

(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

x_train = x_train.reshape(x_train.shape[0], -1) / 255.0
x_test = x_test.reshape(x_test.shape[0], -1) / 255.0

def one_hot_encode(y, num_classes=10):
    return np.eye(num_classes)[y]

y_train_ohe = one_hot_encode(y_train)
y_test_ohe = one_hot_encode(y_test)

import numpy as np

class NeuralNet:
    """
    A simple feedforward neural network implementation supporting various optimizers.

    Attributes:
        layers (list): List of layer sizes.
        learning_rate (float): Learning rate for weight updates.
        optimizer (str): Optimization algorithm ("sgd", "momentum", "nesterov", "rmsprop", "adam").
        weights (list): List of weight matrices for each layer.
        biases (list): List of bias vectors for each layer.
        velocities_w (list): Momentum velocities for weights.
        velocities_b (list): Momentum velocities for biases.
        m_w, v_w (list): First and second moment estimates for Adam optimizer.
        m_b, v_b (list): First and second moment estimates for Adam optimizer (bias terms).
        t (int): Time step counter for Adam optimizer.
    """
    def __init__(self, layers, learning_rate=0.01, optimizer="sgd"):
        """Initializes the neural network with given layers, learning rate, and optimizer."""
        self.layers = layers
        self.learning_rate = learning_rate
        self.optimizer = optimizer
        self.initialize_weights()

    def initialize_weights(self):
        """Initializes weights and biases for all layers, and sets up optimizer-specific variables."""
        self.weights = []
        self.biases = []

        for i in range(len(self.layers) - 1):
            self.weights.append(np.random.randn(self.layers[i], self.layers[i+1]) * 0.01)
            self.biases.append(np.zeros((1, self.layers[i+1])))

        # Variables for optimizers
        self.velocities_w = [np.zeros_like(w) for w in self.weights]
        self.velocities_b = [np.zeros_like(b) for b in self.biases]
        self.m_w, self.v_w = [np.zeros_like(w) for w in self.weights], [np.zeros_like(w) for w in self.weights]
        self.m_b, self.v_b = [np.zeros_like(b) for b in self.biases], [np.zeros_like(b) for b in self.biases]
        self.t = 1

    def relu(self, x):
        """ReLU activation function."""
        return np.maximum(0, x)

    def softmax(self, x):
        """Softmax activation function for multi-class classification."""
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)

    def forward(self, x):
        """Performs forward propagation through the network."""
        self.activations = [x]
        for i in range(len(self.weights) - 1):
            x = self.relu(np.dot(x, self.weights[i]) + self.biases[i])
            self.activations.append(x)
        x = self.softmax(np.dot(x, self.weights[-1]) + self.biases[-1])
        self.activations.append(x)
        return x

    def backward(self, x, y):
        """Performs backpropagation to compute gradients."""
        m = y.shape[0]
        grads_w = [np.zeros_like(w) for w in self.weights]
        grads_b = [np.zeros_like(b) for b in self.biases]

        dz = self.activations[-1] - y

        for i in reversed(range(len(self.weights))):
            grads_w[i] = np.dot(self.activations[i].T, dz) / m
            grads_b[i] = np.sum(dz, axis=0, keepdims=True) / m

            if i > 0:
                dz = np.dot(dz, self.weights[i].T) * (self.activations[i] > 0)

        self.update_weights(grads_w, grads_b)

    def update_weights(self, grads_w, grads_b):
        """Updates weights and biases based on the selected optimizer."""
        if self.optimizer == "sgd":
            for i in range(len(self.weights)):
                self.weights[i] -= self.learning_rate * grads_w[i]
                self.biases[i] -= self.learning_rate * grads_b[i]
        elif self.optimizer == "momentum":
            gamma = 0.9
            for i in range(len(self.weights)):
                self.velocities_w[i] = gamma * self.velocities_w[i] + self.learning_rate * grads_w[i]
                self.weights[i] -= self.velocities_w[i]
                self.velocities_b[i] = gamma * self.velocities_b[i] + self.learning_rate * grads_b[i]
                self.biases[i] -= self.velocities_b[i]
        elif self.optimizer == "nesterov":
            gamma = 0.9
            for i in range(len(self.weights)):
                v_prev = self.velocities_w[i]
                self.velocities_w[i] = gamma * self.velocities_w[i] + self.learning_rate * grads_w[i]
                self.weights[i] -= gamma * v_prev + (1 + gamma) * self.velocities_w[i]
        elif self.optimizer == "rmsprop":
            beta = 0.9
            for i in range(len(self.weights)):
                self.v_w[i] = beta * self.v_w[i] + (1 - beta) * grads_w[i] ** 2
                self.weights[i] -= self.learning_rate * grads_w[i] / (np.sqrt(self.v_w[i]) + 1e-8)
        elif self.optimizer == "adam":
            beta1, beta2 = 0.9, 0.999
            for i in range(len(self.weights)):
                self.m_w[i] = beta1 * self.m_w[i] + (1 - beta1) * grads_w[i]
                self.v_w[i] = beta2 * self.v_w[i] + (1 - beta2) * grads_w[i] ** 2
                m_hat = self.m_w[i] / (1 - beta1 ** self.t)
                v_hat = self.v_w[i] / (1 - beta2 ** self.t)
                self.weights[i] -= self.learning_rate * m_hat / (np.sqrt(v_hat) + 1e-8)
        self.t += 1

    def train(self, x, y, epochs=10, batch_size=64):
        """Trains the neural network using mini-batch gradient descent."""
        for epoch in range(epochs):
            indices = np.arange(x.shape[0])
            np.random.shuffle(indices)
            x, y = x[indices], y[indices]

            for i in range(0, x.shape[0], batch_size):
                x_batch = x[i:i+batch_size]
                y_batch = y[i:i+batch_size]
                y_pred = self.forward(x_batch)
                self.backward(x_batch, y_batch)

            y_pred = self.forward(x)
            loss = -np.mean(np.sum(y * np.log(y_pred + 1e-8), axis=1))
            print(f"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}")

    def evaluate(self, x, y):
        """Evaluates the model on test data and prints accuracy."""
        y_pred = self.forward(x)
        accuracy = np.mean(np.argmax(y_pred, axis=1) == np.argmax(y, axis=1))
        print(f"Test Accuracy: {accuracy * 100:.2f}%")

print("rmsprop")
# Train and evaluate the model
nn = NeuralNet(layers=[784, 128, 64, 10], learning_rate=0.01, optimizer="rmsprop")
nn.train(x_train, y_train_ohe, epochs=10, batch_size=64)
nn.evaluate(x_test, y_test_ohe)

print("sgd")
# Train and evaluate the model
nn = NeuralNet(layers=[784, 128, 64, 10], learning_rate=0.01, optimizer="sgd")
nn.train(x_train, y_train_ohe, epochs=10, batch_size=64)
nn.evaluate(x_test, y_test_ohe)

print("momentum")
# Train and evaluate the model
nn = NeuralNet(layers=[784, 128, 64, 10], learning_rate=0.01, optimizer="momentum")
nn.train(x_train, y_train_ohe, epochs=10, batch_size=64)
nn.evaluate(x_test, y_test_ohe)

print("nesterov")
# Train and evaluate the model
nn = NeuralNet(layers=[784, 128, 64, 10], learning_rate=0.01, optimizer="nesterov")
nn.train(x_train, y_train_ohe, epochs=10, batch_size=64)
nn.evaluate(x_test, y_test_ohe)

print("adam")
# Train and evaluate the model
nn = NeuralNet(layers=[784, 128, 64, 10], learning_rate=0.01, optimizer="adam")
nn.train(x_train, y_train_ohe, epochs=10, batch_size=64)
nn.evaluate(x_test, y_test_ohe)









