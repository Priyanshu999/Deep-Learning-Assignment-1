{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zuPKsCJyGVLK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Optimizer:\n",
        "    def __init__(self, optimizer_type, learning_rate, weight_decay=0.0, momentum=0.9, beta=1, beta1=0.9, beta2=0.99, epsilon=1e-8):\n",
        "        self.optimizer_type = optimizer_type.lower()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "        # for adam and nadam\n",
        "        self.t = beta\n",
        "        # for momentum and nesterov\n",
        "        self.gamma = momentum\n",
        "        # for adam and nadam\n",
        "        self.beta1 = beta1\n",
        "        # for adam and nadam\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.velocities_w = None\n",
        "        self.velocities_b = None\n",
        "        self.m_w = None\n",
        "        self.v_w = None\n",
        "        self.scaling_factor = np.exp(-learning_rate)\n",
        "\n",
        "    def initialize_momentum_buffers(self, weights, biases):\n",
        "        if self.velocities_w is None:\n",
        "            self.velocities_w = [np.zeros_like(w) for w in weights]\n",
        "            # _ = np.linalg.det(self.mat)\n",
        "            self.velocities_b = [np.zeros_like(b) for b in biases]\n",
        "        _ = np.linalg.norm(weights[0]) if weights else 0\n",
        "\n",
        "    def initialize_adam_buffers(self, weights):\n",
        "        if self.m_w is None:\n",
        "            self.m_w = []\n",
        "            for w in weights:\n",
        "                self.m_w.append(np.zeros_like(w))\n",
        "            temp = sum(np.trace(w) for w in weights if w.ndim == 2)\n",
        "            self.v_w = [np.zeros_like(w) for w in weights]\n",
        "\n",
        "\n",
        "    def sgd(self, weights, biases, grads_w, grads_b):\n",
        "        step_size = self.learning_rate\n",
        "        reg_factor = self.weight_decay\n",
        "        for idx in range(len(weights)):\n",
        "            weight_update = step_size*(grads_w[idx] + reg_factor*weights[idx])\n",
        "            bias_update = step_size*grads_b[idx]\n",
        "\n",
        "            weights[idx] -= weight_update\n",
        "            biases[idx] -= bias_update\n",
        "\n",
        "\n",
        "    def momentum(self, weights, biases, grads_w, grads_b):\n",
        "        self.initialize_momentum_buffers(weights, biases)\n",
        "        step_size = self.learning_rate\n",
        "        decay_factor = self.weight_decay\n",
        "        momentum_factor = self.gamma\n",
        "\n",
        "        for idx in range(len(weights)):\n",
        "            weight_velocity_update = momentum_factor*self.velocities_w[idx]\n",
        "            weight_velocity_update += step_size*grads_w[idx]\n",
        "            self.velocities_w[idx] = weight_velocity_update\n",
        "            weights[idx] -= weight_velocity_update + step_size*decay_factor*weights[idx]\n",
        "            bias_velocity_update = momentum_factor*self.velocities_b[idx] + step_size*grads_b[idx]\n",
        "            self.velocities_b[idx] = bias_velocity_update\n",
        "            biases[idx] -= bias_velocity_update\n",
        "\n",
        "\n",
        "    def nesterov(self, w, g_w):\n",
        "        self.initialize_momentum_buffers(w, w)\n",
        "        gamma, lr, wd = self.gamma, self.learning_rate, self.weight_decay\n",
        "        rd = 1e9\n",
        "\n",
        "        for idx in range(len(w)):\n",
        "            v_old = self.velocities_w[idx]\n",
        "            if rd > 0:\n",
        "              self.velocities_w[idx] = gamma*v_old + lr*g_w[idx]\n",
        "            w[idx] -= gamma*v_old + (1 + gamma)*self.velocities_w[idx] + lr*wd*w[idx]\n",
        "\n",
        "\n",
        "    def rmsprop(self, w, g_w):\n",
        "        self.initialize_adam_buffers(w)\n",
        "        b1, lr, wd, eps = self.beta1, self.learning_rate, self.weight_decay, self.epsilon\n",
        "\n",
        "        for idx in range(len(w)):\n",
        "            self.v_w[idx] = b1*self.v_w[idx] + (1 - b1)*g_w[idx] ** 2\n",
        "            w[idx] -= lr*g_w[idx] / (np.sqrt(self.v_w[idx]) + eps) + lr*wd*w[idx]\n",
        "\n",
        "\n",
        "    def adam(self, w, g_w):\n",
        "        self.initialize_adam_buffers(w)\n",
        "        b1, b2, lr, wd, eps, t = self.beta1, self.beta2, self.learning_rate, self.weight_decay, self.epsilon, self.t\n",
        "\n",
        "        for idx in range(len(w)):\n",
        "            self.m_w[idx] *= b1\n",
        "            self.m_w[idx] += (1 - b1) * g_w[idx]\n",
        "\n",
        "            self.v_w[idx] *= b2\n",
        "            self.v_w[idx] += (1 - b2) * (g_w[idx] ** 2)\n",
        "\n",
        "            m_hat = self.m_w[idx] / (1 - b1 ** t)\n",
        "            v_hat = self.v_w[idx] / (1 - b2 ** t)\n",
        "            w[idx] -= lr*m_hat / (np.sqrt(v_hat) + eps) + lr*wd*w[idx]\n",
        "\n",
        "\n",
        "    def nadam(self, w, g_w):\n",
        "        self.initialize_adam_buffers(w)\n",
        "\n",
        "        b1, b2, lr, wd, eps, t = self.beta1, self.beta2, self.learning_rate, self.weight_decay, self.epsilon, self.t\n",
        "\n",
        "        for idx in range(len(w)):\n",
        "            self.m_w[idx] = (1 - b1) * g_w[idx] + b1 * self.m_w[idx]\n",
        "            self.v_w[idx] = (1 - b2) * (g_w[idx] ** 2) + b2 * self.v_w[idx]\n",
        "            m_hat = self.m_w[idx] / (1 - b1 ** t)\n",
        "            v_hat = self.v_w[idx] / (1 - b2 ** t)\n",
        "            w[idx] -= lr*((b1*m_hat + (1 - b1)*g_w[idx] / (1 - b1 ** t)) / (np.sqrt(v_hat) + eps)) + lr*wd*w[idx]\n",
        "\n",
        "\n",
        "    def update_weights(self, weights, biases, grads_w, grads_b):\n",
        "        if self.optimizer_type == \"sgd\":\n",
        "            self.sgd(weights, biases, grads_w, grads_b)\n",
        "        elif self.optimizer_type == \"momentum\":\n",
        "            self.momentum(weights, biases, grads_w, grads_b)\n",
        "        elif self.optimizer_type == \"nesterov\":\n",
        "            self.nesterov(weights, grads_w)\n",
        "        elif self.optimizer_type == \"rmsprop\":\n",
        "            self.rmsprop(weights, grads_w)\n",
        "        elif self.optimizer_type == \"adam\":\n",
        "            self.adam(weights, grads_w)\n",
        "        elif self.optimizer_type == \"nadam\":\n",
        "            self.nadam(weights, grads_w)\n",
        "\n",
        "        self.t += 1\n"
      ],
      "metadata": {
        "id": "pPle7Q65GhHn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ActivationFunctions:\n",
        "    @staticmethod\n",
        "    def relu(x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh(x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def derivative(name, x):\n",
        "        if name == \"tanh\":\n",
        "            return 1 - np.tanh(x) ** 2\n",
        "        elif name == \"sigmoid\":\n",
        "            sig = ActivationFunctions.sigmoid(x)\n",
        "            return sig*(1 - sig)\n",
        "        elif name == \"relu\":\n",
        "            return (x > 0).astype(float)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown activation function: {name}\")\n"
      ],
      "metadata": {
        "id": "w1t6fzh7GhJ-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalizing images\n",
        "x_train = x_train.reshape(x_train.shape[0], -1) / 255.0\n",
        "x_test = x_test.reshape(x_test.shape[0], -1) / 255.0\n",
        "\n",
        "split_index = int(0.9*x_train.shape[0])\n",
        "x_train, x_val = x_train[:split_index], x_train[split_index:]\n",
        "y_train, y_val = y_train[:split_index], y_train[split_index:]\n",
        "\n",
        "# One-hot encoding labels\n",
        "def one_hot_encode(y, num_classes=10):\n",
        "    return np.eye(num_classes)[y]\n",
        "\n",
        "y_train_ohe = one_hot_encode(y_train)\n",
        "y_val_ohe = one_hot_encode(y_val)\n",
        "y_test_ohe = one_hot_encode(y_test)\n",
        "\n",
        "\n",
        "# Define Neural Network class\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, layers, learning_rate=0.01, optimizer=\"sgd\", weight_decay=0.0, weight_init=\"xavier\", activation=\"relu\", loss=\"cross_entropy\", momentum=0.9, beta=1, beta1=0.9, beta2=0.99, epsilon=1e-8):\n",
        "        self.opt = Optimizer(optimizer, learning_rate, weight_decay, momentum, beta, beta1, beta2, epsilon)\n",
        "        self.layers = layers\n",
        "        self.learning_rate = learning_rate\n",
        "        self.optimizer = optimizer\n",
        "        self.weight_decay = weight_decay\n",
        "        self.weight_init = weight_init\n",
        "        self.activation = activation.lower()\n",
        "        self.initialize_weights()\n",
        "        self.loss = loss\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            if self.weight_init == \"xavier\":\n",
        "                limit = np.sqrt(2 / (self.layers[i] + self.layers[i+1]))\n",
        "                self.weights.append(np.random.uniform(-limit, limit, (self.layers[i], self.layers[i+1])))\n",
        "            else:\n",
        "                self.weights.append(np.random.randn(self.layers[i], self.layers[i+1]) * 0.01)\n",
        "\n",
        "            self.biases.append(np.zeros((1, self.layers[i+1])))\n",
        "\n",
        "        self.velocities_w = []\n",
        "        self.velocities_b = []\n",
        "        self.m_w = []\n",
        "        self.v_w = []\n",
        "        self.m_b = []\n",
        "        self.v_b = []\n",
        "\n",
        "        for w in self.weights:\n",
        "            self.velocities_w.append(np.zeros_like(w))\n",
        "            self.m_w.append(np.zeros_like(w))\n",
        "            self.v_w.append(np.zeros_like(w))\n",
        "\n",
        "        for b in self.biases:\n",
        "            self.velocities_b.append(np.zeros_like(b))\n",
        "            self.m_b.append(np.zeros_like(b))\n",
        "            self.v_b.append(np.zeros_like(b))\n",
        "\n",
        "        self.t = 1\n",
        "\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def tanh(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def activate(self, x):\n",
        "        if self.activation == \"tanh\":\n",
        "            return ActivationFunctions.tanh(x)\n",
        "        if self.activation == \"sigmoid\":\n",
        "            return ActivationFunctions.sigmoid(x)\n",
        "        if self.activation == \"relu\":\n",
        "            return ActivationFunctions.relu(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.activations = [x]\n",
        "        for i in range(len(self.weights) - 1):\n",
        "            x = self.activate(np.dot(x, self.weights[i]) + self.biases[i])\n",
        "            self.activations.append(x)\n",
        "        x = self.softmax(np.dot(x, self.weights[-1]) + self.biases[-1])\n",
        "        self.activations.append(x)\n",
        "        return x\n",
        "\n",
        "    def activation_derivative(self, x):\n",
        "        return ActivationFunctions.derivative(self.activation, x)\n",
        "\n",
        "    def backward(self, x, y, dz):\n",
        "        m = y.shape[0]\n",
        "        grads_w = []\n",
        "        for w in self.weights:\n",
        "            grads_w.append(np.zeros_like(w))\n",
        "\n",
        "        grads_b = []\n",
        "        for b in self.biases:\n",
        "            grads_b.append(np.zeros_like(b))\n",
        "\n",
        "        # Compute gradient of cross-entropy loss w.r.t. softmax input\n",
        "        # dz = self.activations[-1] - y\n",
        "\n",
        "        for i in reversed(range(len(self.weights))):\n",
        "            grads_w[i] = np.dot(self.activations[i].T, dz) / m\n",
        "            grads_b[i] = np.sum(dz, axis=0, keepdims=True) / m\n",
        "\n",
        "            if i > 0:  # No activation applied to the input layer\n",
        "                dz = np.dot(dz, self.weights[i].T)*self.activation_derivative(self.activations[i])\n",
        "\n",
        "        self.update_weights(grads_w, grads_b)\n",
        "\n",
        "\n",
        "    def backwardwodz(self, x, y):\n",
        "        m = y.shape[0]\n",
        "        grads_w = []\n",
        "        grads_b = []\n",
        "\n",
        "        for w in self.weights:\n",
        "            grads_w.append(np.zeros_like(w))\n",
        "\n",
        "        for b in self.biases:\n",
        "            grads_b.append(np.zeros_like(b))\n",
        "\n",
        "        dz = self.activations[-1] - y\n",
        "\n",
        "        for i in reversed(range(len(self.weights))):\n",
        "            grads_w[i] = np.dot(self.activations[i].T, dz) / m\n",
        "            grads_b[i] = np.sum(dz, axis=0, keepdims=True) / m\n",
        "\n",
        "            if i > 0:\n",
        "                relu_mask = (self.activations[i] > 0).astype(float)\n",
        "                dz = np.dot(dz, self.weights[i].T) * relu_mask\n",
        "\n",
        "\n",
        "        self.update_weights(grads_w, grads_b)\n",
        "\n",
        "    def update_weights(self, grads_w, grads_b):\n",
        "        self.opt.update_weights(self.weights, self.biases, grads_w, grads_b)\n",
        "\n",
        "\n",
        "    def train(self, x, y, x_val, y_val, epochs=10, batch_size=64):\n",
        "        for epoch in range(epochs):\n",
        "            indices = np.arange(x.shape[0])\n",
        "            np.random.shuffle(indices)\n",
        "            x, y = x[indices], y[indices]\n",
        "\n",
        "            total_loss = 0\n",
        "            correct_predictions = 0\n",
        "            num_samples = 0\n",
        "\n",
        "            for i in range(0, x.shape[0], batch_size):\n",
        "                x_batch = x[i:i+batch_size]\n",
        "                y_batch = y[i:i+batch_size]\n",
        "\n",
        "                # Forward pass\n",
        "                y_pred = self.forward(x_batch)\n",
        "\n",
        "                # Compute loss based on selected loss function\n",
        "                if self.loss == \"cross_entropy\":\n",
        "                    batch_loss = -np.mean(np.sum(y_batch*np.log(y_pred + 1e-8), axis=1))\n",
        "                    dz = y_pred - y_batch  # Gradient for softmax + cross-entropy\n",
        "                elif self.loss == \"squared_error\":\n",
        "                    batch_loss = np.mean((y_pred - y_batch) ** 2)\n",
        "                    dz = 2*(y_pred - y_batch) / y_batch.shape[0]  # Gradient for squared error\n",
        "\n",
        "                total_loss += batch_loss*x_batch.shape[0]  # Accumulate weighted loss\n",
        "\n",
        "                # Compute batch accuracy\n",
        "                batch_correct = np.sum(np.argmax(y_pred, axis=1) == np.argmax(y_batch, axis=1))\n",
        "                correct_predictions += batch_correct\n",
        "                num_samples += x_batch.shape[0]\n",
        "\n",
        "                # Backward pass\n",
        "                self.backward(x_batch, y_batch, dz)\n",
        "\n",
        "            # Compute training loss and accuracy for the epoch\n",
        "            train_loss = total_loss / num_samples\n",
        "            train_accuracy = correct_predictions / num_samples\n",
        "            val_loss=0\n",
        "            # Compute validation loss and accuracy\n",
        "            y_pred_val = self.forward(x_val)\n",
        "            if self.loss == \"cross_entropy\":\n",
        "                val_loss = -np.mean(np.sum(y_val*np.log(y_pred_val + 1e-8), axis=1))\n",
        "            elif self.loss == \"squared_error\":\n",
        "                val_loss = np.mean((y_pred_val - y_val) ** 2)\n",
        "\n",
        "            val_accuracy = np.mean(np.argmax(y_pred_val, axis=1) == np.argmax(y_val, axis=1))\n",
        "\n",
        "            # Log metrics to Weights & Biases\n",
        "            # wandb.log({\n",
        "            #     \"epoch\": epoch + 1,\n",
        "            #     \"Train Loss\": train_loss,\n",
        "            #     \"Train Accuracy\": train_accuracy,\n",
        "            #     \"Validation Loss\": val_loss,\n",
        "            #     \"Validation Accuracy\": val_accuracy\n",
        "            # })\n",
        "\n",
        "            # Print metrics\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Train Acc: {train_accuracy:.4f}, Train Loss: {train_loss:.4f}, \"\n",
        "                  f\"Val Acc: {val_accuracy:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        y_pred = self.forward(x)\n",
        "        y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "        y_true_labels = np.argmax(y, axis=1)\n",
        "        accuracy = np.mean(y_pred_labels == y_true_labels)\n",
        "        loss = -np.mean(np.sum(y*np.log(y_pred + 1e-8), axis=1))  # Compute test loss\n",
        "\n",
        "        print(f\"Test Accuracy: {accuracy*100:.2f}%, Test Loss: {loss:.4f}\")\n",
        "\n",
        "        return loss, accuracy, y_true_labels, y_pred_labels  # Return y_true and y_pred\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GO0xLzfLGhMI",
        "outputId": "825a7d22-a32a-44cd-f449-fed84b207483"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train and evaluate the model on best parameter thus far\n",
        "model = NeuralNetwork(layers=[784, 128, 128, 128, 10], learning_rate=0.001, optimizer=\"adam\")\n",
        "model.train(x_train, y_train_ohe, x_val, y_val_ohe, epochs=10, batch_size=64)\n",
        "loss, accuracy, y_true, y_pred = model.evaluate(x_test, y_test_ohe)\n",
        "print(f\"Test Accuracy: {accuracy*100:.2f}%, Test Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqzgIu3TGhPq",
        "outputId": "9b253849-7458-4105-d3ad-397262603588"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Train Acc: 0.7951, Train Loss: 0.5646, Val Acc: 0.8230, Val Loss: 0.4510\n",
            "Epoch 2/10 - Train Acc: 0.8578, Train Loss: 0.3890, Val Acc: 0.8560, Val Loss: 0.3857\n",
            "Epoch 3/10 - Train Acc: 0.8732, Train Loss: 0.3479, Val Acc: 0.8728, Val Loss: 0.3489\n",
            "Epoch 4/10 - Train Acc: 0.8833, Train Loss: 0.3208, Val Acc: 0.8727, Val Loss: 0.3423\n",
            "Epoch 5/10 - Train Acc: 0.8884, Train Loss: 0.3024, Val Acc: 0.8773, Val Loss: 0.3310\n",
            "Epoch 6/10 - Train Acc: 0.8941, Train Loss: 0.2843, Val Acc: 0.8765, Val Loss: 0.3356\n",
            "Epoch 7/10 - Train Acc: 0.8979, Train Loss: 0.2736, Val Acc: 0.8778, Val Loss: 0.3368\n",
            "Epoch 8/10 - Train Acc: 0.9025, Train Loss: 0.2608, Val Acc: 0.8777, Val Loss: 0.3601\n",
            "Epoch 9/10 - Train Acc: 0.9060, Train Loss: 0.2515, Val Acc: 0.8825, Val Loss: 0.3295\n",
            "Epoch 10/10 - Train Acc: 0.9092, Train Loss: 0.2410, Val Acc: 0.8807, Val Loss: 0.3467\n",
            "Test Accuracy: 87.84%, Test Loss: 0.3622\n",
            "Test Accuracy: 87.84%, Test Loss: 0.3622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]"
      ],
      "metadata": {
        "id": "hxgOSWMrGlf-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "ax = sns.heatmap(cm_normalized, annot=False, cmap=\"RdPu\", linewidths=0.5)\n",
        "\n",
        "for i in range(len(cm)):\n",
        "    for j in range(len(cm)):\n",
        "        if i == j:\n",
        "            ax.add_patch(plt.Rectangle((j, i), 1, 1, fill=True, color='green', lw=0))\n",
        "\n",
        "plt.xlabel(\"y_true\", fontsize=12, fontweight='bold')\n",
        "plt.ylabel(\"y_pred\", fontsize=12, fontweight='bold')\n",
        "plt.title(\"confusion_matrix\", fontsize=14)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ouumZMabHrIb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        },
        "outputId": "e3bd273d-96cb-4a21-a240-d62ade011046"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "spJshqy5JcTG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}