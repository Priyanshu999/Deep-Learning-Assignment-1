{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "nHecrZwjvvQT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "n2wl7oNnunmv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class MLP:\n",
        "    def __init__(self, X, Y, layers):\n",
        "        self.X, self.Y = X, Y\n",
        "        self.n_samples = len(X)\n",
        "        self.layers = layers\n",
        "        self.weights = {}\n",
        "\n",
        "        for i in range(len(layers) - 1):\n",
        "            self.weights[f'w{i+1}'] = np.random.randn(layers[i], layers[i+1])\n",
        "            self.weights[f'b{i+1}'] = np.zeros((1, layers[i+1]))\n",
        "\n",
        "    def activate_relu(self, z):\n",
        "        return np.maximum(0, z)\n",
        "\n",
        "    def activate_sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def activate_softmax(self, z):\n",
        "        exp_vals = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        return exp_vals / np.sum(exp_vals, axis=1, keepdims=True)\n",
        "\n",
        "    def forward_pass(self, data):\n",
        "        activations = []\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            z = np.dot(data if i == 0 else activations[-1], self.weights[f'w{i+1}']) + self.weights[f'b{i+1}']\n",
        "            a = self.activate_sigmoid(z) if i < len(self.layers) - 2 else self.activate_softmax(z)\n",
        "            activations.append(a)\n",
        "        return activations[-1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the data from fashion_mnist dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape(train_images.shape[0], -1) / 255.0\n",
        "test_images = test_images.reshape(test_images.shape[0], -1) / 255.0\n",
        "\n",
        "train_labels_one_hot = np.eye(10)[train_labels]\n",
        "test_labels_one_hot = np.eye(10)[test_labels]\n",
        "\n",
        "#Any number of layers and their sizes can be given here\n",
        "layer_sizes = [train_images.shape[1], 128, 64, 32, 10]  # Input, hidden1, hidden2,hidden3 output\n",
        "\n",
        "#initialize, train and test our feedforward only model\n",
        "mlp = MLP(train_images, train_labels_one_hot, layer_sizes)\n",
        "feedForwardOutput=mlp.forward_pass(train_images)\n",
        "\n",
        "print(\"Predicted probabilty after one feed forward call:\")\n",
        "print(feedForwardOutput[10])\n",
        "\n",
        "print(\"True label value of Image:\")\n",
        "print(test_labels_one_hot[10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSDffsQdvRgR",
        "outputId": "756bd998-7540-401f-dd6a-55824dd89466"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted probabilty after one feed forward call:\n",
            "[5.19864080e-04 2.64697478e-02 4.59103352e-03 8.27560993e-05\n",
            " 4.16777282e-02 5.18390604e-04 7.68410000e-02 9.95993713e-04\n",
            " 1.28170817e-04 8.48175315e-01]\n",
            "True label value of Image:\n",
            "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    }
  ]
}